\documentclass{article} % Класс печатного документа

% для поддержки русского языка
\usepackage[T2A]{fontenc} % поддержка специальных русских символов
\usepackage[utf8]{inputenc} % Кодировка исходного текста - utf8
\usepackage[english,russian]{babel} % Поддержка языка - русского с английским
\usepackage{indentfirst} % Отступ в первом абзаце

\usepackage{hyperref} % Для вставки гиперссылок
% \usepackage{listings} % Для вставки кусков кода
\usepackage{graphicx} % Вставка изображений
\usepackage{subfig} % Изображения друг напротив друга
\usepackage{float} % Для точного позиционирования картинок
\usepackage[justification=centering]{caption} % Для центрирования подписей

\include{pythonstyle} % для красивого оформления python кода

% путь к папке с изображениями
\graphicspath{{./figs/}}

\title{Отчёт 10\\
Мультиклассовая классификация\\
с помощью методов Random Forest (RF)\\
и Gradient Boosting (GB)} % заголовок документа
\author{Свичкарев А.\,В.} % Автор документа
\date{\today} % Текущая дата

\begin{document} % Конец преамбулы, начало текста

\maketitle % Печатает заголовок, список авторов и дату

\section{Цель}
Изучить способы решения задач мультиклассовой классификации
данных с применением методов Random Forest (RF) и Gradient Boosting (GB).

\section{Задание №1}
Написать программу построения модели классификации данных
glass.data.csv методом случайного из файла
леса деревьев решений (Random Forest) и
визуализации деревьев решений.
Построить и визуализировать частные модели деревьев,
на основе которых построить модель предсказания классов данных.
Привести значение минимальной ошибки (Missclassification Error),
график зависимости ошибок (Missclassification Error Rate)
от числа деревьев в ансамбле и матрицу ошибок (Confusion Matrix).
Оценить точность классификации при различных значениях
параметра max\_depth (1,3,5).
\bigskip

Реализация на основе кода программы из Приложения.

\clearpage
Примеры частных моделей деревьев:
\begin{figure}[H]
	\centering
	\subfloat[Первое дерево]{\includegraphics[width=0.5\textwidth]{tree1Ex1.png}}
	\hfill
	\subfloat[Второе дерево]{\includegraphics[width=0.5\textwidth]{tree2Ex1.png}}
    \caption{Графические построения деревьев глубины 1}
\end{figure}
\bigskip

Значения минимальной ошибки (Missclassification Error Rate)
от числа деревьев в ансамбле и матрица ошибок (Confusion Matrix).
при различных величинах глубины деревьев:
\lstinputlisting{./ex1_out.txt}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{mer1depth}
    \caption{График зависимости ошибок от числа деревьев в ансамбле
    (Missclassification Error Rate) при глубине дерева 1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{varImp1depth}
    \caption{Относительная важность различных атрибутов
    (Variable Importance) при глубине дерева 1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{mer3depth}
    \caption{График зависимости ошибок от числа деревьев в ансамбле
    (Missclassification Error Rate) при глубине дерева 3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{varImp3depth}
    \caption{Относительная важность различных атрибутов
    (Variable Importance) при глубине дерева 3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{mer5depth}
    \caption{График зависимости ошибок от числа деревьев в ансамбле
    (Missclassification Error Rate) при глубине дерева 5}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{varImp5depth}
    \caption{Относительная важность различных атрибутов
    (Variable Importance) при глубине дерева 5}
\end{figure}

При увеличение глубины деревьев ---
ошибка уменьшается.
При увеличение числа деревьев в ансамбле ---
ошибка уменьшается, но больше 500 деревьев
не улучшает результата.

\clearpage
\section{Задание №2}
Выполнить классификацию данных из файла glass.data.csv
методом методом Gradient Boosting и визуализировать деревья решений.
Привести результаты классификации данных.
Оценить точность классификации при различных значениях параметра
max\_depth (1,3,5).

Сравнить эффективность двух методов (RF и GB).
\bigskip

Реализация на основе кода программы из Приложения.
\bigskip

Примеры частных моделей деревьев:
\begin{figure}[H]
	\centering
	\subfloat[Первое дерево]{\includegraphics[width=0.5\textwidth]{tree1Ex2.png}}
	\hfill
	\subfloat[Второе дерево]{\includegraphics[width=0.5\textwidth]{tree2Ex2.png}}
    \caption{Графические построения деревьев глубины 1}
\end{figure}
\bigskip

Значения минимальной ошибки (Missclassification Error Rate)
от числа деревьев в ансамбле и матрица ошибок (Confusion Matrix).
при различных величинах глубины деревьев:
\lstinputlisting{./ex1_out.txt}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{perfGB1depth.png}
    \caption{График зависимости ошибок от числа деревьев в ансамбле
    (Missclassification Error Rate) при глубине дерева 1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{varImpBG1depth}
    \caption{Относительная важность различных атрибутов
    (Variable Importance) при глубине дерева 1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{perfGB3depth.png}
    \caption{График зависимости ошибок от числа деревьев в ансамбле
    (Missclassification Error Rate) при глубине дерева 3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{varImpBG3depth}
    \caption{Относительная важность различных атрибутов
    (Variable Importance) при глубине дерева 3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{perfGB5depth.png}
    \caption{График зависимости ошибок от числа деревьев в ансамбле
    (Missclassification Error Rate) при глубине дерева 5}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{varImpBG5depth}
    \caption{Относительная важность различных атрибутов
    (Variable Importance) при глубине дерева 5}
\end{figure}

При увеличение глубины деревьев ---
ошибка уменьшается, но не значительно.
При увеличение числа деревьев в ансамбле ---
ошибка уменьшается, но больше 100 деревьев
улучшает результат не так существенно.

Random Forest работает значительно дольше,
так как использует большие ансамбли.
Gradient Boosting и Random Forest в общем показывают
схожую эффективность, хотя иногда для одного метода
нужно больше деревьев построить, чем для другого.
Это зависит в большей степени от исходных данных.

\section{Пояснение}
Исходный код доступен по ссылке:
\href{https://github.com/SvichkarevAnatoly/Course-Python-Bioinformatics/tree/master/semester2/task10}
{github.com}

\end{document} % Конец документа
